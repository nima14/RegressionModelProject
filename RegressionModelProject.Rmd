---
title: "RegressionModelProject"
author: "Nima Taqidust"
date: "2/23/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE}
library(dplyr)
library(ggplot2)
```

### Summary

First of all we can look at the data that we want to do the analysis for:
```{r str}
str(mtcars)
```

```{r summary}
summary(mtcars$mpg)
```

### Exploratory Analysis

```{r groupby}
group_by(mtcars,am) %>% summarise(mean(mpg),sd(mpg)) %>% as.data.frame()
```
We can see that there a distance betwwen the mean of two types of the variable "am"

Now we can get help from the boxplot to see the defference between the mgp for automaic and manual transmission:

```{r boxplot}
ggplot(mtcars,aes(x=factor(mtcars$am),y=mtcars$mpg,fill=factor(mtcars$am))) +
  geom_boxplot()+ 
  scale_fill_discrete(name = "Transmission", labels = c("Automatic", "Manual"))+
  xlab("Transmission") + ylab("MPG")
```


We can split the data into two groups to run the T-test: automatic and manual transmission

```{r ttest}
mtcarsAutomatic <- mtcars[mtcars$am==0,]

mtcarsManual <- mtcars[mtcars$am==1,]

t.test(mtcarsAutomatic$mpg,mtcarsManual$mpg,paired = FALSE)
```

As the T-test shows, the mpg variable for automatic and manual transmission are not the same.

Now we can see the relation between all the variables in the dataset:

```{r}
pairs(mtcars$mpg ~ .,data=mtcars)
```


Now we see some strong relations between the variables, for a better understanding we can see the correlation between some variables:

```{r cor}
correlation <- as.data.frame(cor(mtcars$cyl,mtcars$disp))
correlation <- as.data.frame(rbind(correlation, cor(mtcars$cyl,mtcars$hp)))
correlation <- as.data.frame(rbind(correlation,cor(mtcars$cyl,mtcars$wt)))
correlation <- as.data.frame(rbind(correlation,cor(mtcars$qsec,mtcars$drat)))
names(correlation) <- "Correlation"
correlation
```

The results show a signinficant correlation between these variables so it is better not to use all these variables together.

### Regression Models

First of all we can do a linear regression with the independent variable "am" and the dependent variable "mpg":

```{r fit1}
fit1 <- lm(mpg~factor(am),mtcars)
CoefFit1 <- summary(fit1)$coef
summary(fit1)
```
Note that R-squared is 0.3598

From the above we know that the variables "cyl","dis","hp" and "wt" have strong correlations so I added one of them into the model named "cyl":

```{r fit2}
fit2 <- lm(mpg~factor(am)+cyl-1,mtcars)
CoefFit2 <- summary(fit2)$coef
summary(fit2)
```
Now the R-squared is  0.9807 which is way better than before.
We use tha anova function to see if the new variable have an improvement on the model or not:

```{r anova12}
anova(fit1,fit2)
```

It had a significant effect on the model so we keep the "cyl".

Now we add the other variables step by step to see their impact on the model.


```{r fit3}

fit3 <- lm(mpg~factor(am)+cyl+drat-1,mtcars)
anova(fit1,fit2,fit3)
```

```{r fit4}
fit4 <- lm(mpg~factor(am)+cyl+qsec-1,mtcars)
anova(fit1,fit2,fit4)
```

```{r fit5}

fit5 <- lm(mpg~factor(am)+cyl+factor(vs)-1,mtcars)
anova(fit1,fit2,fit5)
```

```{r fit6}

fit6 <- lm(mpg~factor(am)+cyl+factor(gear)-1,mtcars)
anova(fit1,fit2,fit6)
```

```{r fit7}

fit7 <- lm(mpg~factor(am)+cyl+factor(carb)-1,mtcars)
anova(fit1,fit2,fit7)
```

We can see that none of the above variables do not have a significant impact on the model so we ommit them all.

So our model is based on the variables "cyl" and "am".

### Appendix

```{r appendix}
FittedMPGs <- as.integer(predict(fit2))
mtcars <- mutate(mtcars,FittedMPGs)
mtcars

  par(mfrow=c(2,2))
  plot(fit2)

